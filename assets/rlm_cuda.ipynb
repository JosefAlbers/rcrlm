{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7IRL6rbgWrs",
        "outputId": "81480c39-6f86-4358-b7be-7af955ec12be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rcrlm[cuda]\n",
            "  Downloading rcrlm-0.0.1a0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tokenizerz==0.0.2 (from rcrlm[cuda])\n",
            "  Downloading tokenizerz-0.0.2.tar.gz (16 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from rcrlm[cuda]) (4.67.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from rcrlm[cuda]) (4.0.0)\n",
            "Collecting mlx[cuda] (from rcrlm[cuda])\n",
            "  Downloading mlx-0.30.0-cp312-cp312-manylinux_2_35_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting ziglang==0.13.0.post1 (from tokenizerz==0.0.2->rcrlm[cuda])\n",
            "  Using cached ziglang-0.13.0.post1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.musllinux_1_1_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from tokenizerz==0.0.2->rcrlm[cuda]) (3.1.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->rcrlm[cuda]) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets->rcrlm[cuda]) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->rcrlm[cuda]) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->rcrlm[cuda]) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->rcrlm[cuda]) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets->rcrlm[cuda]) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->rcrlm[cuda]) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->rcrlm[cuda]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->rcrlm[cuda]) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets->rcrlm[cuda]) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->rcrlm[cuda]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->rcrlm[cuda]) (6.0.3)\n",
            "Collecting mlx-cuda==0.30.0 (from mlx[cuda]; extra == \"cuda\"->rcrlm[cuda])\n",
            "  Downloading mlx_cuda-0.30.0-py3-none-manylinux_2_35_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting nvidia-cublas-cu12==12.9.* (from mlx-cuda==0.30.0->mlx[cuda]; extra == \"cuda\"->rcrlm[cuda])\n",
            "  Downloading nvidia_cublas_cu12-12.9.1.4-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.9.* (from mlx-cuda==0.30.0->mlx[cuda]; extra == \"cuda\"->rcrlm[cuda])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.* in /usr/local/lib/python3.12/dist-packages (from mlx-cuda==0.30.0->mlx[cuda]; extra == \"cuda\"->rcrlm[cuda]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from mlx-cuda==0.30.0->mlx[cuda]; extra == \"cuda\"->rcrlm[cuda]) (2.27.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->rcrlm[cuda]) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->rcrlm[cuda]) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->rcrlm[cuda]) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->rcrlm[cuda]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->rcrlm[cuda]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->rcrlm[cuda]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->rcrlm[cuda]) (2025.11.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->tokenizerz==0.0.2->rcrlm[cuda]) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->rcrlm[cuda]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->rcrlm[cuda]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->rcrlm[cuda]) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->rcrlm[cuda]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->rcrlm[cuda]) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->rcrlm[cuda]) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->rcrlm[cuda]) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->rcrlm[cuda]) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->rcrlm[cuda]) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->rcrlm[cuda]) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->rcrlm[cuda]) (1.17.0)\n",
            "Using cached ziglang-0.13.0.post1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.musllinux_1_1_x86_64.whl (80.9 MB)\n",
            "Downloading mlx_cuda-0.30.0-py3-none-manylinux_2_35_x86_64.whl (62.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.9.1.4-py3-none-manylinux_2_27_x86_64.whl (581.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.2/581.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (89.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rcrlm-0.0.1a0-py3-none-any.whl (17 kB)\n",
            "Downloading mlx-0.30.0-cp312-cp312-manylinux_2_35_x86_64.whl (653 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m654.0/654.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tokenizerz\n",
            "  Building wheel for tokenizerz (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tokenizerz: filename=tokenizerz-0.0.2-py3-none-any.whl size=262955 sha256=196662e3737ab38d36024dd51212cf7e2190023518f157b36df650c420a6ebb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/fc/cb/a183a06781bea678fff8ee279447dcc7702ff1c94d36377265\n",
            "Successfully built tokenizerz\n",
            "Installing collected packages: ziglang, nvidia-cuda-nvrtc-cu12, nvidia-cublas-cu12, mlx, tokenizerz, mlx-cuda, rcrlm\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.9.0+cu126 requires nvidia-cublas-cu12==12.6.4.1; platform_system == \"Linux\", but you have nvidia-cublas-cu12 12.9.1.4 which is incompatible.\n",
            "torch 2.9.0+cu126 requires nvidia-cuda-nvrtc-cu12==12.6.77; platform_system == \"Linux\", but you have nvidia-cuda-nvrtc-cu12 12.9.86 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mlx-0.30.0 mlx-cuda-0.30.0 nvidia-cublas-cu12-12.9.1.4 nvidia-cuda-nvrtc-cu12-12.9.86 rcrlm-0.0.1a0 tokenizerz-0.0.2 ziglang-0.13.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install rcrlm[cuda]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rcrlm import load, infer, train\n",
        "\n",
        "m = load()\n",
        "_ = infer('wtf', **m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypz30Forggbm",
        "outputId": "c22d81c7-5ad2-4cdc-9318-e827910637df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'models/Qwen3-0.6B/config.json' already exists. Skipping.\n",
            "File 'models/Qwen3-0.6B/tokenizer_config.json' already exists. Skipping.\n",
            "File 'models/Qwen3-0.6B/model.safetensors' already exists. Skipping.\n",
            "File 'models/Qwen3-0.6B/tokenizer.json' already exists. Skipping.\n",
            "<think>\n",
            "Okay, the user just said \"wtf\". I need to respond appropriately. First, I should acknowledge their confusion. Maybe say something like \"Oh no! You're confused? Let me know!\" That's friendly and helpful.\n",
            "\n",
            "Next, I should ask them what they're confused about. It's important to keep the conversation open so they can provide more details. Maybe offer assistance in their case. \n",
            "\n",
            "I should also make sure to keep\n",
            "\n",
            "┌────────────────────────────── Inp 000 ──────────────────────────────┐\n",
            "<|im_start|>user\n",
            "wtf<|im_end|>\n",
            "<|im_start|>assistant\n",
            "└─────────────────────────────────────────────────────────────────────┘\n",
            "┌────────────────────────────── Out 000 ──────────────────────────────┐\n",
            "<think>\n",
            "Okay, the user just said \"wtf\". I need to respond appropriately. First, I should acknowledge their confusion. Maybe say something like \"Oh no! You're confused? Let me know!\" That's friendly and helpful.\n",
            "\n",
            "Next, I should ask them what they're confused about. It's important to keep the conversation open so they can provide more details. Maybe offer assistance in their case. \n",
            "\n",
            "I should also make sure to keep the tone light and not too formal. Avoid any\n",
            "└─────────────────────────────────────────────────────────────────────┘\n",
            "=== Benchmarks ===\n",
            "Prompt processing: 84.1 tokens/sec (10 tokens in 0.1s)\n",
            "Token generation: 52.5 tokens/sec (100 tokens in 1.9s)\n"
          ]
        }
      ]
    }
  ]
}